[toc]

# Ceph基本概念

## 什么是Ceph

Ceph 是一个分布式存储系统，可提供高性能、高可用性和可扩展性的存储解决方案。它被设计为在普通硬件上运行，并通过将数据分布在多个节点上实现可靠性和弹性。以下是一些关于 Ceph 的重要信息：

1. 架构：Ceph 采用 RADOS（可靠自动分布对象存储）架构。它由多个对象存储服务器（OSD）节点组成，每个节点存储和处理数据块。元数据服务器（MDS）用于处理文件系统操作，而监视器（MON）维护集群状态和配置信息。
2. 功能：Ceph 提供了对象存储、块存储和文件存储的功能。对象存储通过使用 RESTful API 和对象存储网关（RGW）来存储和检索数据。块存储提供类似于传统磁盘的块设备访问。文件存储使用 Ceph 文件系统（CephFS）提供分布式文件系统的功能。
3. 可靠性和冗余：Ceph 使用数据冗余和复制来确保数据的可靠性和可用性。数据块会被多个 OSD 节点复制，以防止节点故障时数据的丢失。
4. 可扩展性：Ceph 具有横向扩展的能力，可以轻松添加新的 OSD 节点来增加存储容量和性能。它还支持动态数据平衡，可以自动将数据均匀分配到集群中的各个节点上。
5. 社区支持：Ceph 是一个开源项目，由社区开发和维护。它具有活跃的用户和开发者社区，提供文档、邮件列表、论坛和 IRC 渠道等支持资源。
6. 集成和生态系统：Ceph 可以与各种存储和计算平台集成，如 OpenStack、Kubernetes、Proxmox VE、Apache Hadoop 等。它还提供了一些工具和库，用于简化管理和与 Ceph 集成的开发。

这只是 Ceph 的一些基本信息，它具有更多复杂的特性和配置选项。如果您想要深入了解 Ceph，建议查阅官方文档和参考资料。

## Ceph集群

Ceph 集群是由多个节点组成的分布式存储系统。每个节点负责存储和处理数据，共同组成一个可靠、可扩展、高性能的存储集群。

Ceph 集群的核心组件是对象存储服务器（OSD），它们是实际存储数据的节点。每个 OSD 都有自己的存储设备，通过网络连接到其他节点。OSD 节点负责处理读写请求、复制数据、修复故障和执行数据恢复等操作。

另一个重要的组件是监视器（Monitor），它们是集群的管理节点。监视器负责维护集群的状态和配置信息，跟踪 OSD 和元数据服务器（MDS）的状态，监控故障和变化，并提供集群配置、映射和定位服务。

Ceph 集群还包括元数据服务器（MDS），它是 Ceph 文件系统（CephFS）的组件。MDS 负责处理文件系统的元数据操作，维护文件和目录结构、权限信息和文件属性等元数据，并处理文件系统的元数据操作，如创建、删除、重命名等。

集群中的节点通过网络进行通信和协调，使用 CRUSH 算法（Controlled Replication Under Scalable Hashing）来确定数据的存放位置。CRUSH 算法通过 CRUSH Map 配置文件指导数据在集群中的分布和复制，以实现数据的冗余和可靠性。

Ceph 集群可以根据需求进行扩展，通过添加新的 OSD 节点来增加存储容量和性能。它还支持动态数据平衡，以保持数据的均衡分布，并自动进行数据修复和恢复。

总之，Ceph 集群是一个由 OSD、MDS、Monitor 等组成的分布式存储系统，通过分布式存储和冗余机制提供可靠性和可扩展性，并通过管理节点和算法来实现高性能和数据平衡。

## OSD

OSD（Object Storage Daemon）是 Ceph 存储集群中的核心组件之一，它负责存储和处理数据对象。每个 OSD 都是一个独立的进程，运行在独立的节点上，通过网络连接与其他 OSD 节点和监视器节点通信。

以下是 OSD 的详细介绍：

1. 存储设备：每个 OSD 节点通常都有自己的存储设备，可以是硬盘、SSD 或其他类型的存储介质。OSD 使用这些存储设备来存储数据对象。
2. 数据复制和冗余：Ceph 通过数据复制和冗余机制来提供数据的可靠性和高可用性。每个数据对象在集群中都有多个副本，这些副本存储在不同的 OSD 节点上，以防止数据丢失和故障。
3. 数据分布和定位：Ceph 使用 CRUSH 算法（Controlled Replication Under Scalable Hashing）来确定数据对象的存放位置。CRUSH 算法基于 CRUSH Map 配置文件，根据 OSD 节点的权重和拓扑结构决定数据对象的分布和复制策略。
4. 数据读写和处理：当客户端请求读取或写入数据时，OSD 负责处理这些请求。对于读取请求，OSD 从存储设备中检索数据对象，并将其返回给客户端。对于写入请求，OSD 将数据对象写入存储设备，并将其复制到其他 OSD 节点以确保数据的冗余和可靠性。
5. 数据修复和恢复：当一个 OSD 节点发生故障或下线时，集群会自动进行数据修复和恢复操作。其他 OSD 节点会接管故障节点的数据对象，并将其复制到其他 OSD 节点上，以保持数据的冗余和可靠性。
6. 健康监测和报告：OSD 定期向监视器节点发送自身的状态和健康信息。监视器节点会监控和报告 OSD 节点的健康状况，以便及时发现和处理故障。

总之，OSD 是 Ceph 集群中负责存储和处理数据对象的组件。它通过数据复制和冗余机制提供数据的可靠性和高可用性，使用 CRUSH 算法确定数据的分布和复制策略，并负责处理数据读写请求、数据修复和恢复等操作。

## Monitor

Monitor（监视器）是 Ceph 存储集群中的重要组件之一，它负责管理和监控集群的状态、配置信息和元数据。下面是 Monitor 的详细介绍：

1. 集群状态管理：Monitors 负责跟踪和管理整个 Ceph 集群的状态。它们监控 OSD 和其他组件的健康状况、负载情况和性能指标，并将这些信息汇报给其他组件和管理员。
2. 配置管理：Monitors 存储集群的全局配置信息，例如存储池的配置参数、副本数、CRUSH Map 配置等。它们也负责更新和分发配置更改，以确保集群中各组件的一致性。
3. 元数据管理：Monitors 存储集群的元数据，包括对象名称空间、目录结构、文件系统元数据等。它们负责处理元数据操作，例如创建、重命名、删除对象等，并将元数据信息提供给客户端。
4. 故障检测和恢复：Monitors 使用心跳机制和时间线机制来检测 OSD 的故障和恢复。它们还负责维护 OSD 的状态映射表，以跟踪 OSD 的上线和下线情况，并调整数据分布以实现数据的冗余和可靠性。
5. 高可用性：为了提供高可用性，Ceph 集群通常会有多个 Monitor 节点。这些节点通过选举机制选择一个主 Monitor（Leader）和若干备用 Monitor（Follower）。主 Monitor 负责处理客户端请求和协调集群操作，备用 Monitor 用于故障转移和容错。
6. 集群通信：Monitors 之间通过消息传递来进行通信和协调。它们使用 Paxos 算法来达成一致性决策，并保持集群状态的一致性。

总之，Monitor 是 Ceph 集群中负责管理和监控集群状态、配置信息和元数据的组件。它们负责跟踪 OSD 和其他组件的健康状况、管理集群的全局配置、处理元数据操作、检测故障和恢复、提供高可用性和进行集群通信。

## MGR

mgr（Manager）是Ceph集群中的一个组件，负责管理和监控集群的状态、性能和元数据。mgr提供了集群管理、监控、调度和策略控制等功能，以帮助管理员更好地管理Ceph集群。

下面是mgr的一些详细介绍：

1. 状态管理：mgr负责收集和维护关于集群、OSD、PG、Pool和其他资源的状态信息。它会监视集群的运行状况，并将这些信息提供给其他组件和管理员。管理员可以通过mgr来获取有关集群状态的信息，如集群健康状况、容量利用率等。
2. 性能监控：mgr会收集有关集群和存储设备的性能指标，如吞吐量、延迟、负载等。它可以将这些指标用于性能分析和故障排除，并提供实时的性能监控图表。管理员可以通过mgr来查看集群的性能指标，并根据需要进行调整和优化。
3. 元数据管理：mgr还负责管理Ceph集群的元数据，如PG映射、CRUSH地图、配置信息等。它可以维护元数据的一致性，并确保集群的配置和拓扑信息是最新的。管理员可以通过mgr来查询和修改集群的元数据，并进行相关配置的管理。
4. 策略控制：mgr提供了一些策略和规则控制的功能，可以根据管理员的需求对集群进行自动化操作和调度。例如，可以通过mgr来实现自动的数据迁移、平衡负载、故障转移等操作，以优化集群的性能和可靠性。
5. 插件支持：mgr支持插件机制，可以根据需求加载和使用各种扩展插件。这些插件可以增加mgr的功能，如添加新的监控指标、提供额外的管理功能等。管理员可以根据需要选择和配置合适的插件，以满足特定的需求。

mgr是Ceph集群中一个重要的管理组件，它提供了集群管理、监控和控制的核心功能。通过mgr，管理员可以更好地了解集群的状态和性能，并进行灵活的管理和调度，以满足不同的需求和场景。

## MDS

MDS（Metadata Server）是 Ceph 存储集群中的组件之一，负责管理和处理文件系统的元数据操作。下面是 MDS 的详细介绍：

1. 文件系统支持：MDS 提供了 Ceph 存储集群上的分布式文件系统（CephFS）。CephFS 允许用户在集群中创建和管理文件，提供了类似传统文件系统（如 ext4）的接口和功能。
2. 元数据管理：MDS 负责管理文件系统的元数据，包括文件和目录的名称、权限、所有权、时间戳等信息。它们维护文件系统的目录结构，处理文件和目录的创建、修改、删除等操作。
3. 元数据缓存：为了提高性能，MDS 使用元数据缓存来加速元数据访问。当客户端访问文件系统时，MDS 会将最常用的元数据缓存在内存中，以减少磁盘访问和提高响应速度。
4. 元数据分布：CephFS 的元数据可以根据 CRUSH 算法分布在多个 MDS 节点上，以实现元数据的负载均衡和扩展性。这样可以避免单点故障和瓶颈，并提高文件系统的并发处理能力。
5. 并发访问和一致性：MDS 使用锁和事务机制来实现并发访问和一致性控制。它们确保多个客户端可以同时访问文件系统，并保持元数据的一致性，避免冲突和数据损坏。
6. 故障恢复和容错：当 MDS 节点发生故障或下线时，CephFS 会自动进行故障恢复和容错操作。其他 MDS 节点会接管故障节点的任务，并保持文件系统的正常运行。
7. 高可用性：为了提供高可用性，CephFS 通常会有多个 MDS 节点。这些节点通过选举机制选择一个主 MDS（Active）和若干备用 MDS（Standby）。主 MDS 负责处理客户端请求和协调文件系统操作，备用 MDS 用于故障转移和容错。

总之，MDS 是 Ceph 存储集群中负责管理和处理文件系统的元数据操作的组件。它们提供了分布式文件系统的支持，管理元数据和目录结构，处理文件和目录的创建、修改、删除等操作。MDS 使用元数据缓存、分布和一致性控制来提高性能和可靠性，支持故障恢复和高可用性。

## Object

在 Ceph 存储系统中，Object（对象）是 Ceph 存储集群中的核心数据单元。下面是关于 Ceph Object 的详细介绍：

1. 数据单元：Ceph Object 是 Ceph 存储系统中最小的数据单元。每个 Object 都有一个唯一的标识符，称为 Object ID，用于在 Ceph 存储集群中唯一标识该对象。
2. 数据和元数据：Ceph Object 不仅包含实际的数据，还包含相关的元数据信息。元数据包括 Object 的大小、修改时间、访问权限等。这些元数据信息存储在 Ceph 存储集群中的 OSD（Object Storage Daemon）中。
3. 分布式存储：Ceph Object 存储在 Ceph 存储集群的 OSD 中，而不是集中存储在某个中心位置。这意味着每个 Object 可以在集群中的不同 OSD 上进行复制和分布，以实现高可用性和容错性。
4. 对象存储命名空间：Ceph Object 存储在一个对象存储命名空间中。命名空间提供了逻辑组织和管理 Object 的方式，类似于目录结构。可以使用命名空间和 Object ID 来定位和访问特定的 Object。
5. 对象存储接口：Ceph 提供了多种对象存储接口，使用户可以方便地读取、写入和操作 Object。常用的接口有 RADOS Gateway（提供 S3 和 Swift 兼容接口）、librados（C语言接口）和 radosgw-admin（命令行接口）等。

总之，Ceph Object 是 Ceph 存储系统中的核心数据单元，它包含了实际的数据和相关的元数据信息。它以分布式的方式存储在集群中的 OSD 中，并通过对象存储命名空间和相应的接口进行管理和访问。这种分布式对象存储的方式使得 Ceph 具备高可用性、容错性和可扩展性。

## Pool

在 Ceph 存储系统中，Pool（存储池）是一种逻辑组织和管理存储资源的方式。下面是有关 Ceph Pool 的详细介绍：

1. 数据组织：Pool 是 Ceph 存储集群中的数据组织单元。每个 Pool 都有一个唯一的名称，用于在集群中唯一标识该存储池。
2. 存储资源：Pool 是由多个 OSD（Object Storage Daemon）组成的，这些 OSD 可以是物理硬盘、SSD 或云存储服务。Pool 中的每个 OSD 负责存储和管理数据。
3. 数据分布：Ceph 使用 CRUSH 算法将数据均匀地分布到不同的 OSD 上。CRUSH 算法可以根据 OSD 的容量和性能特征，以及其他自定义规则，动态选择合适的 OSD 存储数据。
4. 数据复制：Pool 可以配置副本数来决定数据的复制方式。Ceph 支持在不同 OSD 上创建多个副本，以提高数据的可用性和容错性。Ceph 还支持自动故障转移和数据重平衡，以确保数据的一致性和可靠性。
5. 数据保护：Pool 可以配置快照来对数据进行保护。快照是 Pool 中对象的只读副本，可以用于恢复误删除的数据或回滚到先前的状态。
6. 容量管理：Pool 可以分配一定的存储容量给不同的用户、应用程序或数据类型。这样可以根据需求进行容量管理和配额控制，确保每个用户或应用程序都有足够的存储空间。
7. 数据访问控制：Pool 可以配置访问控制策略，以控制用户或应用程序对存储池中数据的访问权限。这可以通过身份验证、权限管理和加密等方式来实现。

总之，Ceph Pool 是 Ceph 存储系统中逻辑上组织和管理存储资源的方式。它提供了数据组织、存储资源管理、数据分布、数据复制、数据保护、容量管理和数据访问控制等功能。通过合理配置和管理存储池，可以提供高可用性、容错性和性能的存储服务。

## RGW

Ceph RGW（Rados Gateway）是 Ceph 存储系统中的一个组件，是一个提供对象存储服务的 RESTful 网关接口。下面是有关 Ceph RGW 的详细介绍：

1. 对象存储：Ceph RGW 提供了类似于 Amazon S3 和 OpenStack Swift 的对象存储服务。它允许用户通过 RESTful 接口访问和管理存储在 Ceph 存储集群中的对象数据。对象可以是任意大小的文件，每个对象都有一个唯一的标识符。
2. 架构：Ceph RGW 使用 Ceph RADOS（Reliable Autonomic Distributed Object Store）作为后端存储。它与其他组件（如 OSD 和 MON）紧密配合，以提供高可用性、容错性和性能。
3. 访问控制：Ceph RGW 支持基于用户、角色和访问策略的访问控制。可以通过创建和管理用户和角色，并定义访问策略来控制对存储桶和对象的访问权限。
4. 存储桶管理：Ceph RGW 允许用户创建和管理存储桶。存储桶是用于组织和管理对象的容器。用户可以对存储桶设置权限、访问日志、生命周期策略等。
5. 数据复制：Ceph RGW 支持将数据复制到不同的 Ceph 存储集群或其他存储系统，以提供数据的冗余性和灾备能力。复制可以通过配置异地多副本策略来实现。
6. 多租户支持：Ceph RGW 支持多租户环境，允许在同一个 RGW 部署中为不同的用户或组织提供独立的对象存储服务。
7. 安全性：Ceph RGW 支持 HTTPS 和加密功能，以确保数据在传输和存储过程中的安全性。
8. 监控和日志：Ceph RGW 提供了丰富的监控和日志功能，可以实时监视存储使用情况、性能指标和错误日志，以便进行故障排除和性能优化。

总之，Ceph RGW 是 Ceph 存储系统中用于提供对象存储服务的组件。它通过 RESTful 接口允许用户访问和管理存储在 Ceph 存储集群中的对象数据。Ceph RGW 提供了访问控制、存储桶管理、数据复制、多租户支持、安全性以及监控和日志等功能，以满足用户的对象存储需求。

## PG

PG（Placement Group）是 Ceph 存储集群中数据的逻辑分组单位。它决定了数据在 OSD（Object Storage Device）之间的分布和复制策略。以下是 Ceph PG 的详细介绍：

PG 是 Ceph 存储集群中数据的逻辑单位。每个 PG 包含多个对象（objects），它们被分布在 OSD 存储节点上。PG 用于实现数据的分布和冗余。

Ceph 使用 CRUSH 算法将数据块映射到 PG。CRUSH 算法根据存储集群的拓扑信息和 OSD 的状态，决定将数据存储在哪些 OSD 上。每个 PG 可以配置多个副本（replicas），以提供数据的冗余和可靠性。

PG 的副本可以分布在不同的 OSD 上，以提供故障容忍能力。当一个 OSD 失效时，Ceph 可以从其他 OSD 上的副本中恢复数据。

PG 的数量和分布对 Ceph 集群的性能和可靠性有重要影响。较少的 PG 数量会导致负载不平衡和性能下降，而较多的 PG 数量会导致管理开销增加。

您可以使用命令 `ceph pg stat` 查看每个 PG 的状态和统计信息。这将显示 PG 的 ID、状态、副本数、对象数等信息。另外，命令 `ceph pg dump` 可以显示 PG 的详细信息，包括 PG 的状态、所属的 OSD、数据分布等。

根据实际需求，您可以调整 PG 的数量和分布来优化 Ceph 存储集群的性能和可靠性。这可以通过修改 CRUSH 映射规则和存储池的配置来实现。

***作者有话说：比如我创建了一个pool，里面存储了100g的数据，我当前节点有3个osd，那我设置PG数量就是3，这样每个osd会获得一个其中一份数据，类似于ES里面的分片。但是这里必须配置为2的冥等，也就是2，4，8，16，32，类似这样的数，否则会告警***

## PGG

PGP（Primary Placement Group）是Ceph存储集群中的一个重要概念，用于控制数据的分片和分布。PGP在PG（Placement Group）的基础上进行了一层逻辑的抽象。

在Ceph中，PG用于将数据在存储集群中进行分片和分布。每个对象都会分配到一个PG中进行存储。PG是Ceph中数据存储和复制的基本单位，用于实现数据的分布和冗余。每个PG都有一个主OSD和若干个副本OSD，用于数据的冗余备份。

PGP是对PG的再次抽象，用于控制PG的分布和映射。PGP数量应该与PG数量相等，除非你有特殊需求。PGP的作用是确保PG在存储集群中的均匀分布和负载均衡。

PGP的数量决定了Ceph集群的负载均衡效果。如果PGP数量过少，可能会导致部分OSD负载过重，影响性能；如果PGP数量过多，可能会导致PG的分布不均匀，也会影响性能。

在创建pool时，需要指定PGP数量并与PG数量相等。可以使用`ceph osd pool calchashpg <pg-num>`命令来计算推荐的PG数量，并将其与PGP数量保持一致。这样可以确保PG在存储集群中的均匀分布和负载均衡。

***作者有话说：PGG可以理解就是就是pool里面的具体文件应该落到那个PG的算法，正常情况下应该和PG相等，如果不相等也会告警。***
