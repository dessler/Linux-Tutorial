# Ceph的容量问题

Ceph的容量分为2部分，一部分是Ceph的整个集群的统计，一部分是Pool的统计，这2个部分由于统计差异，会导致出现部分差异。

![image-20240123165654680](.Ceph df/image-20240123165654680.png)

```
#查看每个osd及总量,总共9个节点，每个节点100GiB 合计900GiB。
ceph osd df
```

![image-20240123165823130](.Ceph df/image-20240123165823130.png)

```
#900G是底层物理磁盘，按照3副本来计算，可以存储大概300G数据，实际用了大概10G,那就应该还剩余290G，这里显示只有270G，这里的数据到那里去了?
ceph df
```

```
#而且我们还可以看到，最大可用空间(MAX AVAIL),出现了3个不同的值。
#前面的271G  AVAIL*副本数/0.95  ------   861/3*0.95≈271      3副本利用率大概30%
#后面的2个pool并不是采用副本模式，而是采用的纠错码。                
#后面406G的是，EC2+2  ------ 861/2*0.95≈406                  纠错码2+2利用率大概50%
#后面406G的是，EC4+2  ------ 861*0.66*0.95≈542               纠错码4+2利用率大概66%
```



***作者有话说：这里的0.95是根据多个Ceph集群得到的一个比例，具体的算法是很复杂的，有兴趣可以下去研究。***